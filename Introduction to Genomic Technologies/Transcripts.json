[
  {
    "index": 1,
    "title": "1 Overview of Genomics",
    "content": "Genomics is the study of genomes, which means looking at all the DNA inside an organism’s cells. When we focus on humans, we’re interested in understanding how our genome shapes who we are, how our bodies develop, and why we differ from one another. Even though people look very different, our DNA sequences are about 99.9% the same. That tiny 0.1% difference is what creates all the variety in traits like height, eye color, and susceptibility to diseases like cancer. The genome acts like a biological instruction manual that starts from a single fertilized egg and guides the development of a whole person, including how cells become specialized types like neurons or skin cells, even though every cell contains the same DNA.\n\nCancer is a key example of how genomics helps us understand disease. Cancer happens when cells lose control over their division and start growing uncontrollably. This usually results from mutations, which are changes in the DNA sequence. Mutations can occur because of damage to DNA or errors during the process of copying DNA when cells divide. Although cells are very good at copying DNA accurately, mistakes happen occasionally, and some of these mistakes can cause cancer by affecting genes that control cell division.\n\nThe way information flows in biology is often described by the central dogma, which says that DNA is transcribed into RNA, and RNA is translated into proteins. Proteins do most of the work in cells, from building structures to carrying out chemical reactions. But this flow isn’t one-way. Proteins and other factors can influence which genes are turned on or off, creating feedback loops that help cells behave differently even though they have the same DNA.\n\nTo study genomes, scientists use sequencing technology to read the letters of DNA—A, C, G, and T. Sequencing has become much faster and cheaper over the last couple of decades, allowing researchers to sequence entire genomes quickly. This has opened up many possibilities, like studying the genomes of cancer cells or looking at genetic variation across populations.\n\nUnderstanding some basic cell biology helps make sense of genomics. All living things fall into three domains: eukaryotes, archaea, and bacteria. Eukaryotes, which include humans, have cells with a nucleus that holds the DNA. Prokaryotes, which include bacteria and archaea, don’t have a nucleus; their DNA floats more freely inside the cell. In human cells, DNA is organized into 23 pairs of chromosomes, one set from each parent. Cells divide through mitosis, where DNA is copied so each new cell gets a full set of chromosomes. Humans are diploid, meaning we have two copies of each chromosome. When producing sperm or egg cells, a special division called meiosis happens, which reduces the chromosome number by half and includes recombination, where chromosomes from each parent exchange pieces. This recombination creates new combinations of genes and is a major source of genetic diversity.\n\nThe three main molecules in molecular biology are DNA, RNA, and proteins. DNA stores genetic information using four nucleotides: adenine, guanine, cytosine, and thymine. These pair up in a specific way—A with T, and G with C—forming the double helix structure. RNA is similar to DNA but usually single-stranded and uses uracil instead of thymine. RNA is made by copying DNA in a process called transcription. Messenger RNA carries the genetic code to the cell’s protein-making machinery. Proteins are chains of amino acids that fold into complex shapes and perform most cellular functions. The sequence of amino acids in a protein is determined by reading RNA in groups of three nucleotides called codons, each coding for one amino acid or a stop signal.\n\nThe Human Genome Project was a large international effort that started in 1989 to sequence the entire human genome. Early sequencing was slow and expensive, so the project broke the genome into manageable pieces called bacterial artificial chromosomes, sequenced those, and then assembled the full genome. In 1995, a new method called whole genome shotgun sequencing was used to sequence a bacterial genome by randomly sequencing many small fragments and assembling them with computers. This method was later used by a private company, Celera Genomics, to sequence the human genome faster, sparking a race with the public project. Both groups announced draft human genomes in 2000, and the final draft was published in 2001. The project revealed that humans have about 20,000 to 23,000 protein-coding genes, far fewer than earlier estimates, and that only about 1.5% of the genome codes for proteins.\n\nDNA is extremely long but fits inside the nucleus because it wraps around proteins called histones, forming structures that coil tightly into chromosomes. The genome contains many repetitive sequences, which can be tandem repeats (repeated one after another) or interspersed repeats (scattered throughout the genome). These repeats can make sequencing and analysis more difficult because short DNA reads may match multiple places. Genes in eukaryotes often contain introns, non-coding regions that are removed during RNA processing, leaving exons, which code for proteins. Alternative splicing allows different combinations of exons to produce multiple proteins from a single gene.\n\nProteins fold into complex three-dimensional shapes that determine their function. Gene expression is controlled by transcription factors, proteins that bind to DNA near genes and regulate whether those genes are turned on or off. This regulation allows different cell types to express different genes despite having the same DNA. Epigenetics adds another layer of control through chemical modifications like DNA methylation, which can influence gene activity and be passed on when cells divide, but these changes are not inherited between generations.\n\nYour genotype is your complete set of genetic information, while your phenotype is the set of observable traits like eye color, height, or disease status. Some traits follow simple genetic rules, like dominant and recessive inheritance seen in Mendel’s pea experiments. In humans, many traits are more complex and influenced by multiple genes and environmental factors. Large-scale sequencing projects have mapped genetic variation across populations, showing that people from different regions have characteristic patterns of mutations. Genome-wide association studies compare genetic variants in people with and without a trait to find links between specific mutations and diseases or traits. For example, a variant in the HERC2 gene is strongly associated with eye color in Europeans.\n\nGenomic data science combines biology, statistics, and computer science to analyze the huge amounts of data generated by sequencing. The process starts with experimental design, deciding what question to ask and how many samples to collect. Then data is generated by sequencing DNA or RNA, producing millions or billions of short reads. These reads are aligned to a reference genome to identify differences and assemble sequences. Preprocessing and normalization correct for biases and errors in the data. Finally, statistical and computational methods are used to interpret the data and draw biological conclusions. Software development is important for creating tools that handle standardized experiments like RNA sequencing, which measures gene expression levels. Beyond individual genomes, population genomics studies genetic variation across groups to understand disease susceptibility and evolution. Integrative genomics combines multiple data types to get a comprehensive view of biological systems.\n\nIn summary, genomics is about understanding the complete genetic material of organisms and how it shapes biology and disease. Advances in sequencing and data analysis have transformed genomics into a data-rich science, enabling discoveries about how DNA influences life from molecules to populations. Understanding DNA, RNA, proteins, cell biology, and computational methods is key to exploring this field."
  },
  {
    "index": 2,
    "title": "2 Measurement Technology",
    "content": "Today, we’re going to talk about some important tools and techniques used to study DNA, starting with polymerase chain reaction, or PCR. PCR is a method that allows us to make many copies of a specific piece of DNA. This is really useful because most DNA analysis methods, like sequencing, need a lot of identical copies of DNA to work properly. Imagine you have just one tiny piece of DNA, and you want to study it in detail. PCR lets you take that one piece and make millions or even billions of copies in a relatively short time.\n\nThe way PCR works is based on a few simple properties of DNA. DNA is made of two strands that stick together because of base pairing—A pairs with T, and C pairs with G. Also, DNA strands have direction, from what we call the 5’ end to the 3’ end. To copy DNA, PCR uses short pieces of DNA called primers. These primers are designed to match the start and end of the DNA segment you want to copy. When you mix your DNA with these primers, along with DNA polymerase (an enzyme that builds new DNA strands) and the building blocks of DNA (the nucleotides A, C, G, and T), you can start the copying process.\n\nPCR involves cycling through three temperature steps. First, you heat the mixture to separate the two DNA strands. Then you cool it down so the primers can stick to their matching sequences on the single strands. Finally, you raise the temperature a bit so the DNA polymerase can add nucleotides to the primers, making new strands of DNA. After one cycle, you have twice as much DNA as you started with. Repeating this cycle about 30 times can produce billions of copies from just one original molecule. This exponential increase is why it’s called a chain reaction.\n\nNext, let’s talk about next generation sequencing, or NGS. This is a set of modern technologies that let us read the sequence of DNA much faster and cheaper than older methods. Before NGS, the main method was Sanger sequencing, which was slower and could only handle one DNA fragment at a time. NGS changed that by allowing millions of DNA fragments to be sequenced in parallel.\n\nIn NGS, the DNA you want to sequence is first chopped into small pieces. These pieces are attached to a solid surface, like a slide, and then copied many times using PCR right there on the slide. This creates clusters of identical DNA fragments. To read the sequence, special nucleotides that glow different colors when hit by a laser are added one at a time. Each base—A, C, G, or T—has its own color. After adding one base, a picture is taken to see which color shows up at each cluster. Then the chemical block on the nucleotide is removed so the next base can be added. By repeating this cycle, the sequence of each fragment is read base by base.\n\nOne important thing to know about NGS is that errors can happen, especially as the sequencing goes on. Sometimes the enzyme adds an extra base or skips one, causing some fragments in a cluster to get out of sync. This makes the signal less clear and increases the chance of mistakes in the sequence. That’s why the quality of the data usually decreases toward the end of the reads.\n\nNow, what can we do with these sequencing technologies? There are several important applications. One is exome sequencing, which focuses on sequencing just the parts of the genome that code for proteins, called exons. Although exons make up only about 1.5% of the genome, they contain most of the mutations that affect how proteins work. To do this, scientists use special kits that capture only the exon parts of DNA, letting them sequence just those regions. This saves time and money compared to sequencing the whole genome.\n\nAnother application is RNA sequencing, or RNA-seq. This method looks at which genes are active in a cell by sequencing the RNA molecules. Since RNA can’t be sequenced directly, it’s first converted back into DNA using an enzyme called reverse transcriptase. RNA molecules have a special tail made of many adenine bases, called a poly-A tail, which helps scientists capture only the RNA they want to study. RNA-seq gives a snapshot of gene expression, showing which genes are turned on or off in a particular cell or tissue.\n\nChIP-seq is another technique that uses sequencing to find where proteins bind to DNA. Proteins called transcription factors control gene activity by attaching to specific DNA regions. In ChIP-seq, proteins are chemically fixed to DNA, the DNA is broken into fragments, and antibodies are used to pull out the protein-DNA complexes. The DNA attached to these proteins is then sequenced, revealing the binding sites of the proteins.\n\nFinally, there’s bisulfite sequencing, which is used to study DNA methylation, an epigenetic modification that affects gene expression without changing the DNA sequence itself. Methylation happens on cytosine bases. In this method, DNA is split into two samples. One sample is treated with a chemical called bisulfite, which changes unmethylated cytosines into uracils, but leaves methylated cytosines unchanged. By sequencing both samples and comparing them, scientists can tell which cytosines were methylated. This helps understand how gene activity is regulated beyond the DNA sequence.\n\nTogether, these techniques—PCR, NGS, and their various applications—have transformed how we study genetics and molecular biology. They allow us to copy DNA, read its sequence quickly, and explore how genes are regulated and expressed in different cells and conditions. Understanding these tools is key to making sense of modern biological research and the data it produces."
  },
  {
    "index": 3,
    "title": "3 Computing Technology",
    "content": "Computer science is a broad field that covers many different activities related to computers and computation. To make sense of it, we often divide it into three main areas: theory, systems, and applications. Theory deals with understanding what computers can do and what kinds of problems they can solve. Systems focus on the computers themselves—how they work, the software that runs them, like operating systems, and the programming languages we use to write software. Applications are about using computers to solve real-world problems, which can be in science, business, or many other fields. As the field has grown, many applications have become specialized and sometimes are no longer called computer science.\n\nA key skill in computer science is thinking computationally. This means breaking down problems into clear, precise steps that a computer can follow exactly. Computers don’t guess or interpret; they do exactly what they’re told, so instructions must be very clear. Computational thinking is not just about knowing how to program; it’s about understanding how to translate a problem into a form a computer can handle.\n\nWhen we talk about systems, one important part is the operating system. This is the software that manages the computer’s hardware and resources. For example, if you’re using a Mac, underneath it runs an operating system called UNIX, which has been around for decades. The operating system moves data between storage devices and the computer’s memory, loads and runs programs, manages input and output devices like screens and printers, and handles multiple processors so you can run several programs at once without problems.\n\nProgramming languages are how we communicate with computers. Unlike natural languages like English, programming languages are very strict and precise. Each word or symbol has a specific meaning, and the computer executes exactly what the code says. There are many programming languages, such as Python, Perl, C++, and Java, each suited for different tasks. Learning a programming language means learning how to give clear instructions to a computer.\n\nWriting code that works once is not enough. Software engineering is about writing code that is reliable and tested. Good software handles all expected and unexpected cases without crashing or freezing. For example, a program that divides numbers must check that it doesn’t divide by zero, which would cause an error. This kind of careful thinking is especially important when working with large and complex datasets.\n\nBeyond software, computer science also involves hardware—the physical devices that run software. Today, computers are everywhere, from laptops and phones to robots and other devices. Many devices are controlled by embedded computers, and understanding how software interacts with hardware is an important part of the field.\n\nAlgorithms are step-by-step instructions for solving problems. They don’t need computers to exist; a recipe for brownies is an algorithm because it tells you exactly what steps to follow to get the final product. In computer science, algorithms describe how to solve problems clearly and completely.\n\nFor example, if you want to find the highest point in a landscape, one simple algorithm is to look around, find the steepest uphill direction, take a step that way, and repeat until you can’t go any higher. This will find the nearest peak but not necessarily the highest peak overall. This shows that some algorithms find local solutions rather than the best possible one.\n\nSorting is another common problem. One simple sorting method is bubble sort, where you repeatedly compare pairs of items and swap them if they’re in the wrong order, going through the list multiple times until everything is sorted. Bubble sort always works but is slow for large lists. This introduces the idea of efficiency—some algorithms solve problems faster or with less resource use than others.\n\nData structures are ways to organize and store data so it can be accessed and modified efficiently. This is especially important with large datasets like genomic sequences. DNA sequences are long strings of letters, and computers usually store each letter as one byte. But since DNA only has four letters, we can store each letter using just two bits, compressing the data by four times. This kind of efficiency matters when dealing with gigabytes or terabytes of data.\n\nCommon data structures include lists, linked lists, and trees. These structures help keep track of data and relationships between data. In computer memory, every piece of data has an address, and pointers are variables that store these addresses. Using pointers, data structures can link related pieces of data, allowing quick navigation through large datasets.\n\nSometimes, instead of storing every sequence explicitly, we store patterns or probabilistic models. For example, introns in genes have characteristic start and end sequences. By analyzing many introns, we can create a model showing the likelihood of each nucleotide at each position. This compresses the information and helps recognize these patterns in new data.\n\nEfficiency is about how fast and resourcefully an algorithm solves a problem, which is crucial when working with big data. For example, imagine a mail truck delivering mail. A simple approach is to pick up mail for one house, deliver it, return to the warehouse, and repeat. This works but wastes time traveling back and forth. A better approach is to load mail for many houses at once and deliver it in a route that minimizes travel distance before returning. This kind of problem is related to the traveling salesman problem, a classic challenge in computer science.\n\nSoftware engineering is especially important in genomic data science because datasets are huge and complex. Programs must handle all possible cases without crashing or producing misleading results. For example, RNA editing is a biological process where some RNA nucleotides are changed after transcription. Detecting RNA editing involves aligning RNA sequences to DNA sequences and looking for differences. Some researchers found thousands of new RNA editing sites using alignment software, but many of these were due to software errors. This shows that even well-engineered software can produce errors on big datasets, so it’s important to understand how software works and verify results carefully.\n\nComputational biology software transforms raw biological data into meaningful information. Raw data from genome sequencing is just long strings of letters that are hard to interpret. Software pipelines process this data through multiple steps to produce results that biologists can understand and use. For example, in RNA sequencing, one pipeline uses tools to align short RNA reads to the genome, handle reads that span splice junctions, assemble aligned reads into genes, estimate gene expression levels, and compare expression between samples. These tools convert raw data into lists of genes and their expression levels, which can be used to study biological differences.\n\nSoftware tools evolve quickly. Newer versions improve speed and accuracy, and different tools can produce different results. Computational biologists must stay informed about the latest software and understand their strengths and limitations. Sequencing technology and computational biology software are both advancing rapidly. New sequencing machines produce longer and more complex reads, and software must adapt to handle new data types and larger volumes. Using outdated software on new data can lead to incorrect results. Continuous learning and updating software tools are essential for accurate analysis."
  },
  {
    "index": 4,
    "title": "4 Data Science Technology",
    "content": "When we talk about genomic data science, most people immediately think about biology and computer science. Those are the two obvious parts—understanding the biology behind the data and using computers to handle the massive amounts of information. But there’s a third part that’s just as important, and that’s statistics. Statistics often gets overlooked or treated as a side note, but it’s really the backbone that holds everything together. Without good statistics, the conclusions we draw from genomic data can be wrong, misleading, or even harmful.\n\nA good example of why statistics matters comes from a high-profile study that claimed it could use genomic data to predict which chemotherapy treatments would work best for individual cancer patients. This was a big deal because it promised personalized medicine tailored to each person’s genetic makeup. But when other researchers tried to reproduce the results, they ran into serious problems. The original data and code weren’t fully shared, and the analysis itself had mistakes. This wasn’t just a minor issue—it led to clinical trials based on faulty analysis, lawsuits, and a major review by the Institute of Medicine that called for stricter standards in how genomic data is analyzed and shared. This story shows that ignoring statistics or not giving it the attention it deserves can have real consequences, including risks to patients.\n\nOne of the main problems in that case was a lack of transparency. The researchers didn’t share their raw data or the code they used to analyze it, so others couldn’t check or reproduce their work. Reproducibility is a key principle in science—it means that if someone else follows the same steps, they should get the same results. Without access to data and code, reproducibility is impossible. Another problem was a lack of statistical expertise. The prediction models they used were based on incorrect probability calculations, which should have been obvious to someone trained in statistics. They also had study design issues, like running samples on different days in a way that was linked to the outcome, which introduced confounding factors. Plus, their prediction rules weren’t stable—they could give different results just because the analysis was run on a different day. Eventually, statisticians were able to reconstruct the analysis and make it reproducible, but the original conclusions were simply wrong.\n\nThis example highlights why it’s important to involve statisticians early in the research process, not just at the end. Good statistics helps design experiments properly, analyze data correctly, and avoid mistakes that can mislead everyone.\n\nAt the heart of statistics is a simple but powerful idea: we want to learn about a big population, but measuring every individual in that population is usually impossible or too expensive. Instead, we take a smaller sample that represents the population, and then use that sample to make educated guesses about the whole group. This process is called statistical inference. Because the sample is only a part of the population, there’s always some uncertainty in our estimates. Good statistical methods help us quantify that uncertainty so we know how confident we can be in our conclusions.\n\nOne challenge is that the population itself can change over time. For example, Google Flu Trends tried to predict flu outbreaks by analyzing search terms. At first, it worked well, but over time, as people changed how they searched for flu symptoms, the model became less accurate. This shows that understanding what the population really is and how it might shift is important for making reliable predictions.\n\nSharing data properly is another key part of good statistics in genomics. A complete data set for sharing includes four things: the raw data, which is the unprocessed information straight from the experiment; the tidy data, which is cleaned and organized so it’s easy to analyze; a code book that explains what each variable means and how it’s measured; and a recipe, which is a clear set of instructions or scripts that show exactly how the raw data was turned into the tidy data. Without all four, it’s hard for others to reproduce or trust the analysis. Scripts are preferred because they can be run automatically, reducing errors. If scripts aren’t available, very detailed manual instructions are needed, but those are more prone to mistakes.\n\nIf you’re working with genomic data and need help with statistics, there are several ways to get it. Learning some statistics yourself is a great start, and there are many online courses available. If you have specific questions, websites like Cross Validated and Stack Overflow are good places to ask. Some labs hire bioinformaticians who specialize in computational biology, but working alone can be tough without a support network. The best approach is often to collaborate with a center or group that has a team of statisticians and computational biologists who can provide expertise and resources.\n\nWhen analyzing genomic data, it’s important to interact with the data visually. Large data sets can be overwhelming, so summarizing and plotting the data helps reveal patterns, outliers, or problems that numbers alone might miss. Showing the raw data points in plots, rather than just averages or confidence intervals, gives a clearer picture of variability and sample size. Comparing technical replicates—repeated measurements of the same sample—helps assess how reliable the technology is. Different types of plots, like scatter plots or Bland-Altman plots, can highlight different aspects of the data. But be careful to avoid overly complex or flashy plots that look impressive but don’t actually communicate useful information.\n\nGood experimental design is crucial for reliable results. Sample size matters because it affects your ability to detect real differences. Variability comes from three sources: differences between groups you’re comparing, measurement error from the technology, and natural biological variation between individuals. Before starting a study, you should estimate how many samples you need to have a good chance of detecting an effect if it exists. Balanced designs, where groups have roughly equal numbers of samples, and replication, both technical and biological, help ensure your results are trustworthy. Including positive and negative controls is also important to check that your methods are working properly.\n\nStatistical significance is a way to decide if observed differences are likely real or just due to chance. The t-statistic measures how far apart group averages are relative to the variability in the data. The p-value tells you the probability of seeing a difference as big as you did if there really is no difference. A small p-value suggests the difference is unlikely to be random. But it’s important to understand what a p-value is not: it’s not the probability that there is no difference, nor is it the probability that there is a difference. Misinterpreting p-values is common and can lead to wrong conclusions.\n\nIn genomic studies, you often test thousands of hypotheses at once, like checking every gene for differences. This creates a multiple testing problem because some tests will appear significant just by chance. P-values under the null hypothesis are uniformly distributed, meaning about 5% will be below 0.05 even if nothing is really going on. To handle this, statisticians use error rates like the family wise error rate, which controls the chance of any false positives, and the false discovery rate, which controls the expected proportion of false positives among all discoveries. These methods help balance finding true effects while limiting false alarms.\n\nIt’s also important to avoid p-value hacking, which is changing your analysis or data until you get a significant result. This can happen unintentionally but leads to misleading findings. The best way to avoid it is to plan your analysis before looking at the data and stick to that plan. Reporting negative results, even when you don’t find significant differences, is important to avoid bias in the scientific literature.\n\nFinally, confounding and batch effects are common problems in genomic studies. Confounding happens when a third variable influences both the factor you’re studying and the outcome, making it look like there’s a relationship when there isn’t. For example, shoe size might seem related to literacy, but age is the real factor affecting both. In genomics, batch effects occur when samples are processed at different times or with different equipment, causing differences unrelated to biology. These effects can create false signals. To reduce confounding, randomizing sample assignments and stratifying your design to balance known confounders across groups are effective strategies. Good experimental design also includes replication and controls to help separate true biological signals from artifacts.\n\nIn summary, statistics is not just a technical detail in genomic data science—it’s essential for designing experiments, analyzing data correctly, and making reliable conclusions. Paying attention to transparency, reproducibility, variability, significance, multiple testing, and confounding helps avoid mistakes and improves the quality of research. Working interactively with data and collaborating with experts further strengthens your ability to understand complex genomic information."
  }
]